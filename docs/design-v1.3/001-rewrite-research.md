# Building a Production-Quality Markdown Parser in Erlang

The most critical insight for implementing a markdown-rs-inspired parser in Erlang is that **the event-based state machine architecture is non-negotiable** for achieving both CommonMark compliance and linear time complexity. Unlike typical parsers that produce ASTs directly, markdown-rs processes input character-by-character through finite state machines, emitting enter/exit events for every token with complete positional information. This approach, combined with hand-rolled binary pattern matching in Erlang and strategic use of records for internal AST representation, can achieve CommonMark compliance in 6-12 months for a small team, with nested lists and emphasis parsing representing the primary complexity challenges requiring 3-4 weeks each.

## The specifications landscape and what compliance really means

**CommonMark 0.31.2 is your baseline target**, providing an unambiguous specification with over 650 embedded test cases that any production implementation must pass. Released in January 2024, it resolves the ambiguities in John Gruber's original Markdown through formal grammar definitions and a two-stage parsing process: block structure first (containers and leaf blocks), then inline content within blocks. The specification defines critical parsing principles including tab handling (as if replaced by spaces with 4-character tab stops), block structure precedence over inline structure, and the guarantee that any Unicode sequence is valid input.

GitHub Flavored Markdown (GFM) is formally defined as a **strict superset of CommonMark**, adding five extensions without modifying base behavior: tables with pipe syntax and alignment, task lists with checkbox syntax, strikethrough with double tildes, extended autolinks for bare URLs and emails, and tag filtering for security. This supersets relationship is architecturally significant—every valid CommonMark document renders identically in GFM, making GFM the ideal target for maximizing compatibility while supporting practical features.

Pandoc Markdown diverges from this model, optimized for multi-format document conversion rather than HTML output. It adds powerful features like superscripts/subscripts, definition lists, footnotes with `[^1]` syntax, integrated citation systems with CSL, native math support, YAML metadata blocks, and multiple table formats. However, **Pandoc conflicts with CommonMark in several areas**: it requires blank lines before headers and blockquotes (CommonMark doesn't), handles list indentation differently, and processes typography with smart quote conversion by default. For an initial implementation targeting web use cases, focus on CommonMark with GFM extensions; Pandoc support can come later as its ~90% overlap with CommonMark makes it a reasonable extension path.

The test suites provide your validation framework. CommonMark's `spec.txt` contains 650+ tests in a parseable format with markdown input, expected HTML output, section names, and test numbers. The test runner `spec_tests.py` can validate any implementation, and the tests are available as JSON via the `commonmark-spec` npm package. GFM uses the same infrastructure with additional tests for each extension. These aren't just regression tests—they're the formal definition of correct behavior, covering edge cases like `[foo *bar](baz*)` where the emphasis crosses link boundaries.

## How markdown-rs achieves perfect compliance through events

The markdown-rs architecture follows a three-stage pipeline that fundamentally differs from traditional parsers. Input bytes flow through **character code normalization** (converting tabs to virtual spaces, unifying line endings as special negative codes like `M_0003_CRLF`), then through **state machine tokenization** producing an event stream, and finally through **compilation** to either HTML or mdast. This separation is critical: parsing concerns itself only with recognizing structure and emitting events, while compilation handles output formatting.

The event system is the architectural breakthrough that enables 100% CommonMark compliance. Events represent token boundaries with two types: `Enter` marking the start of a construct and `Exit` marking its end. For the input `*hello*`, the parser emits seven events: Enter emphasis, Enter emphasisSequence (the opening `*`), Exit emphasisSequence, Enter emphasisText, Exit emphasisText, Enter emphasisSequence (closing `*`), Exit emphasisSequence, Exit emphasis. Each event carries precise position information with line (1-indexed), column (1-indexed), and byte offset (0-indexed). This granularity provides four key benefits: every byte is accounted for enabling perfect source mapping, streaming processing becomes possible as tokens emit immediately upon recognition, nested content gets resolved through linked tokens where outer constructs mark inner regions for re-parsing, and the language-agnostic event format decouples parsing from output generation.

The state machine operates through **character-by-character processing** without backtracking, guaranteeing O(n) time complexity. States are functions consuming character codes and returning the next state, using an effects API to emit tokens and events. The construct system modularizes this complexity—each Markdown element (headings, lists, emphasis, code blocks) is a separate construct with its own tokenizer function. Constructs register at specific character codes: `42` (asterisk) triggers the attention construct for emphasis/strong, `91` (left bracket) triggers label_start for links and images, `35` (hash) triggers heading_atx. When the parser encounters a character, it looks up registered constructs and attempts them in priority order until one succeeds.

The parser handles multiple Markdown variants through **compile-time configuration** rather than plugins. A `Constructs` struct contains boolean flags for each feature: `heading_atx`, `code_indented`, `gfm_strikethrough`, `gfm_table`, `math_text`, `mdx_jsx_text`, etc. Preset configurations like `Options::gfm()` enable all GitHub features simultaneously. This monolithic approach sacrifices user extensibility but ensures all extensions interact correctly—critical because MDX, for example, must disable indented code blocks (conflicts with JSX indentation), autolinks (HTML `<tag>` vs JSX `<Component>`), and HTML blocks (replaced by JSX). The library achieves this integration while remaining `#![no_std]` compatible, working in embedded environments with just `alloc`.

## Testing strategies that complement specification compliance

While markdown-rs achieves 100% correctness through **650+ CommonMark tests, 1000+ differential tests comparing against cmark/cmark-gfm, comprehensive fuzz testing with cargo-fuzz and honggfuzz, and 100% code coverage**, it notably doesn't use property-based testing. This reveals an important insight: spec compliance tests are sufficient for correctness when you have a formal specification, but property-based testing (PBT) can still add significant value for exploration and catching semantic bugs.

Property-based testing with Erlang's PropEr library offers five high-value property categories for Markdown parsers. **Roundtrip properties** are the most valuable: parsing Markdown to AST, rendering that AST back to Markdown, then parsing again should yield the same AST. This requires implementing both parser and renderer but catches semantic preservation bugs that example tests miss. **Structural invariants** ensure every parsed AST maintains well-formedness properties: no orphaned nodes, proper parent-child relationships, heading depths between 1-6, valid position information, and block elements don't contain invalid children. **Differential testing properties** compare your output against reference implementations like cmark, ensuring semantic compatibility across the ecosystem. **Robustness properties** verify the parser never crashes on any input (even malicious binary garbage) and maintains linear time complexity preventing denial-of-service attacks. **Extension isolation properties** ensure CommonMark-only documents produce identical results whether extensions are enabled or disabled.

The challenge with PBT for Markdown lies in **generator complexity**. Markdown's nested structures require carefully sized generators to prevent exponential explosion. In PropEr, use `?SIZED(Size, generator(Size))` to control depth, implement base cases for recursive structures (`sized_ast(0) -> leaf(); sized_ast(N) -> complex_node(N div 2)`), and bias generation toward common cases with `?FREQUENCY([{10, common_case()}, {1, edge_case()}])`. The payoff comes during shrinking: PropEr automatically minimizes failing examples, turning a 9993-character document that crashes the parser into a 3-line minimal reproduction.

The optimal testing strategy layers five approaches, each catching different bug classes. **Spec compliance tests** (650+ CommonMark examples) form the foundation, running on every commit to ensure basic correctness. **Property-based tests** run with 100-1000 cases per commit for exploration, with extended 10,000+ case runs nightly to find deeper edge cases. **Fuzz testing** runs continuously in the background, feeding random and mutated inputs to find security issues and crash bugs. **Differential testing** runs weekly against cmark/cmark-gfm to catch semantic divergence. **Golden/regression tests** capture real-world documents and bug fixes, preventing known issues from reoccurring. This layered approach provides comprehensive coverage: specs define correctness, PBT explores the space, fuzzing finds security issues, differential testing ensures ecosystem compatibility, and golden tests prevent regressions.

## Erlang implementation patterns for maximum performance

Modern Erlang OTP 26-28 provides critical features that make it surprisingly competitive with Rust for parsing workloads. **OTP 26 introduced major binary construction optimizations** where the JIT emits single-instruction code that constructs binary contents in CPU registers before writing to memory. Type-based JIT optimizations use type information in BEAM files to generate better native code with fewer instructions. OTP 27's binary/bitstring rewrite further improved handling, though full compiler optimization comes in future releases. These improvements mean well-written Erlang binary pattern matching can achieve 100+ MB/s parsing on commodity hardware—sufficient for production use.

The state machine implementation should use **hand-rolled pattern matching rather than gen_statem** for the core tokenizer, reserving gen_statem only for higher-level document state if needed. The critical pattern matches in function heads to enable match context optimization, preventing sub-binary creation on each iteration. Compare the optimized approach `parse(<<Char, Rest/binary>>, Acc) -> parse(Rest, [Char|Acc])` with the pessimized version that matches in the function body, creating a sub-binary every iteration and destroying performance. Use tail recursion exclusively—the compiler optimizes tail calls to loops with no stack consumption, essential when processing megabyte documents. Enable `-compile(bin_opt_info)` to get compiler warnings about suboptimal binary handling.

For AST representation, the optimal approach uses **records internally for parsing, maps externally for API**. Records provide O(1) field access via compile-time tuple indexing, roughly 1.5x faster than small maps for field access, with minimal memory overhead since they're just tagged tuples. Define records for internal nodes like `#ast_node{type, pos, children, meta}` and use pattern matching in function heads for maximum performance. However, records require recompilation when fields change and lack flexibility for extensions. Maps excel at the API boundary: small maps under 32 elements use the compact FLATMAP representation (sorted key tuple shared between instances) with just 5-word overhead, they serialize naturally to JSON for external tools, and they support dynamic structure for variant-specific extensions. The hybrid approach converts records to maps at API boundaries using a `to_mdast/1` function that recursively transforms the internal tree.

Performance optimization focuses on four patterns Erlang handles exceptionally well. **Binary pattern matching** with match contexts keeps the binary reference alive across recursion without creating sub-binaries, enabling efficient sequential processing. **Iolists for accumulation** avoid repeated binary concatenation—instead of `<<Str/binary, " World">>` which copies the entire first binary, build `[<<"Hello">>, <<" ">>, <<"World">>]` and convert once with `iolist_to_binary/1`. **Avoid premature maps** since maps:get/3 with defaults prevents key sharing between instances; instead use map syntax and always use `:=` for updates requiring existing keys. **Resist NIFs unless profiling proves necessity**—modern Erlang's JIT is fast enough that the overhead and risk (NIFs block scheduler threads, must return in <1ms or use yielding, crashes kill the VM) outweigh benefits for most parsing workloads.

## Supporting multiple Markdown variants through configuration

The architecture for supporting CommonMark, GFM, and eventually Pandoc should use **behavior-based variants with configuration dispatch** rather than separate parser implementations. Define a `markdown_variant` behavior specifying callbacks for `name/0` returning the variant atom, `parse_rules/0` returning a map of construct names to parser functions, `extensions/0` listing enabled features, and `options/0` providing default configuration. Concrete implementations like `markdown_gfm` then inherit CommonMark's base rules via `Base = markdown_commonmark:parse_rules()` and merge in extension-specific parsers for tables, strikethrough, and task lists.

Module organization should separate concerns while maintaining cohesion. The top-level structure places `markdown.erl` as the public API, `markdown_tokenizer.erl` for core state machine logic, `markdown_parser.erl` for orchestration, and `markdown_ast.erl` for tree utilities. A `variants/` subdirectory contains `markdown_commonmark.erl`, `markdown_gfm.erl`, and `markdown_pandoc.erl` implementing the variant behavior. An `extensions/` directory holds modular extension implementations like `markdown_ext_tables.erl` that can be mixed into variants. The `renderers/` directory provides output formats: `markdown_html.erl` for HTML generation and `markdown_ast_transform.erl` for AST manipulation.

Configuration should use Erlang's application environment for defaults while allowing runtime override. Store defaults in `app.src` or `sys.config` with `{markdown, [{default_variant, commonmark}, {extensions, [tables, footnotes]}]}` and access via `application:get_env(markdown, default_variant, commonmark)`. The parser dispatch pattern looks up the variant module dynamically: `Module = variant_module(Variant)` mapping `commonmark -> markdown_commonmark`, `gfm -> markdown_gfm`, then calls `Rules = Module:parse_rules()` to get the construct map. This enables single parser core with multiple configurations, avoiding code duplication while supporting clean variant separation.

The extension system requires careful design since extensions can interact. Use a configuration record `#parse_config{variant, features, extensions}` passed through all parsing functions. Check feature flags with `enabled(Feature, Config)` before attempting extension-specific constructs. For complex extensions that fundamentally change parsing rules (like MDX disabling indented code blocks), apply transformations to the base ruleset during configuration: `if mdx_enabled -> disable_constructs([code_indented, autolink, html_flow, html_text])`. This ensures extensions interact correctly, critical for maintaining CommonMark base compatibility when extensions are disabled.

## Implementation roadmap from prototype to production

The critical path to a production parser follows six phases spanning 6-12 months for 1-3 developers. **Phase 0 (Foundation, 2-3 weeks)** establishes the project structure, studies markdown-rs architecture focusing on `src/tokenizer.rs` and `src/construct/*.rs`, defines core data structures for events and state machine context, and sets up the CommonMark test suite. This phase is essential groundwork—rushing here creates architectural debt that compounds later.

**Phase 1 (Basic Infrastructure, 3-4 weeks)** implements the character-by-character processing loop, state transition mechanism, event queue management, and position tracking for line/column/offset. Build the simplest constructs first: thematic breaks testing character counting and state transitions, blank line handling, and paragraphs without inline parsing. The basic compiler converting events to HTML enables validation. These simple constructs validate the state machine approach before tackling complexity—thematic breaks alone exercise the core pattern you'll use for everything.

**Phase 2 (Block Structures, 4-6 weeks)** is where complexity explodes. Start with ATX headings (`# heading`), then indented code blocks (critical for understanding list interaction rules), then fenced code blocks with info strings. Implement block quotes for recursive container logic and lazy continuation. Finally tackle **lists—the hardest part of any Markdown parser**. List implementation requires tracking indentation levels simultaneously, handling mixed list markers (unordered `- ` containing ordered `1. `), determining tight vs loose lists requiring lookahead, and managing the four-space rule where indentation creates code blocks not continuation. Nested lists can take 2-3 weeks alone; John MacFarlane noted "most of the complexity in list item rules is motivated by indented code blocks."

**Phase 3 (Inline Parsing, 4-5 weeks)** implements text processing with character references, entity decoding, and escape sequences, then code spans with backtick matching. The major challenge is **emphasis and strong emphasis requiring 17 rules from the CommonMark spec**. The complexity stems from ambiguous delimiter interpretation (`***` parseable multiple ways), left-flanking vs right-flanking detection, delimiter runs of varying lengths, and nested emphasis resolution order. Implement a delimiter stack (FIFO queue for potential emphasis delimiters) and process inline content in two passes: identify potential delimiters with flanking information, then match delimiters according to precedence rules. After emphasis, implement links and images using inline syntax `[text](url)`, reference syntax `[text][ref]`, shortcut references `[text]`, and autolinks `<url>`.

**Phase 4 (Resolution & Edge Cases, 2-3 weeks)** handles cross-references through two-pass parsing: collect all link reference definitions in the first pass, resolve references in the second with case-insensitive matching. HTML blocks require understanding seven distinct start/end condition pairs defined in the spec, each needing separate state machine logic. Setext headings (underline-style with `===` or `---`) interact with paragraphs in subtle ways. Edge case handling ensures empty documents parse correctly, documents ending mid-construct terminate gracefully, and malformed input recovers without crashes.

**Phase 5 (AST Generation, 2-3 weeks)** builds mdast-compatible syntax trees from the event stream. Define records for all mdast node types (Root, Paragraph, Heading, List, Code, Emphasis, Link, Image, etc.) with position information and child relationships. Implement stack-based tree building that matches Enter/Exit event pairs, maintaining parent-child relationships correctly. AST utilities provide tree traversal, node manipulation, and pretty printing. This phase comes after HTML validation because HTML output catching parsing bugs is easier than debugging AST structure issues.

**Phase 6 (Testing & Optimization, 3-4 weeks)** ensures production quality by porting all 650+ CommonMark tests, adding custom edge cases discovered during development, and implementing fuzz testing with generated random input. Profile hot paths using `eprof` and `fprof` to identify bottlenecks, optimize state transitions and binary handling based on measurements, and reduce memory usage by eliminating unnecessary allocations. Document the API with usage examples, architecture with design decisions explained, and construct system to enable future extensions.

## Timeline realities and complexity management

For a **solo developer**, expect 12-18 months to production-ready quality, with Phase 2 (block structures) taking 8 weeks as lists resist solo debugging. The lack of code review means bugs hide longer, and getting stuck on emphasis parsing or nested list edge cases with no one to consult extends timelines. For **two developers**, timeline compresses to 8-12 months through parallel work on independent constructs (one handling blocks, one handling inlines), pair programming on the hardest parts (lists and emphasis), and continuous code review catching bugs early. For **three developers**, 6-10 months becomes realistic with role specialization: a parser lead handling state machine architecture and hardest problems, a block developer focusing on lists/quotes/code blocks, and an inline developer handling emphasis/links/images.

The hardest problems require focused attention. **Nested lists** rank 9/10 complexity because they require tracking multiple indentation levels simultaneously, handling four-space rules where indentation switches between code blocks and list continuation, and managing tight/loose list determination requiring lookahead. Allocate 2-3 weeks just for lists with comprehensive test coverage for pathological cases like `- Item 1\n    - Nested (4 spaces - is this code or nested list?)`. **Emphasis parsing** ranks 8/10 due to 17 CommonMark rules for ambiguous delimiter interpretation, requiring delimiter stack implementation and two-pass processing. **HTML block detection** ranks 7/10 because seven distinct start/end condition pairs must be recognized without full HTML parsing. **Reference link resolution** ranks 6/10 requiring two-pass document parsing with case-insensitive matching via ETS tables. **Performance pathological cases** rank 7/10 as nested structures can cause exponential blowup—thousands of unclosed `[` brackets must parse in linear time, requiring maximum nesting depth limits (100 levels recommended) and iterative algorithms avoiding recursion.

Apply 30-35% time buffers covering technical complexity (+20%), team coordination if multiple developers (+10%), unknown unknowns (+15%), and testing/bug-fixing (+20%). This isn't padding—CommonMark parsing is deceptively hard. Multiple implementations have CVEs for denial-of-service attacks, the original Markdown.pl had quadratic complexity bugs, and even specification author John MacFarlane calls it a "complicated beast." Build checkpoints for de-risking: at month 2, validate the state machine approach is working (if not, consider simpler regex-based parsing accepting less compliance); at month 4, confirm lists are working (if not, consider temporarily removing nested list support); at month 7, check performance is acceptable (if not, may need NIF implementation for hot paths).

The path to production quality requires understanding that "production ready" means passing all 650+ CommonMark tests with zero failures, maintaining linear O(n) time complexity with no quadratic blowups, generating both HTML and AST output, providing clear error messages with line numbers, including comprehensive documentation with architecture guide, and being fuzz-tested against malicious inputs. Extensions like GFM support, syntax highlighting integration, source map generation, and multiple output formats add 2-3 months beyond this baseline. The investment pays off with a fast, correct, maintainable Markdown parser showcasing Erlang's strengths: pattern matching for tokenization, processes for concurrent document processing if needed, and the BEAM's JIT generating native code competitive with compiled languages for parsing workloads.

Start simple with a throwaway spike parser handling just paragraphs and headings in weeks 1-2, validating Erlang's suitability before committing to architecture. Establish the real project structure in weeks 3-4 with proper state machine core and thematic breaks working perfectly—this establishes the pattern for everything else. By month 2, achieve your first milestone: a working parser for a subset (no lists, no emphasis) that can parse simple blog posts with HTML output. This validates your approach and builds confidence before tackling the exponential complexity of nested structures. The event-based state machine architecture isn't negotiable for CommonMark compliance, but starting simple lets you master the pattern before applying it to the hardest constructs.